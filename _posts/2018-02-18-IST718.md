---
layout: single
title: "IST 718: Big Data Analytics"
categories: Teaching
last_modified_at: 2018-06-28
---

__This is an advanced course__: There seem to be no official pre-requisites
 in the Syracuse University's catalog system for taking this class. 
Most students have already taken _IST 687 - Introduction to Data Science_, 
which is a nice introduction to the field. __However, students will be
expected to know programming in Python or R and have
some background in linear algebra, calculus, probability, and statistics as well__. This means
that even if you register for the class, you might not have the necessary
background to fully take advantage of what this class has to offer.  
If you are in doubt, take the following test, which you should be able to solve relatively
easily  
[<i class="far fa-file-pdf"></i> Preliminary test](/assets/pdf/preliminary_test_ist718.pdf){: .btn .btn--inverse}  
{: .notice--danger}

In the past, I have suggested students go through the following courses
to grasp the basic math required to be a good data scientist:

- **Linear algebra**: This [MIT OCW’s Linear Algebra](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/index.htm) course, which is free  The first couple of lectures cover most you need
- **Calculus**: Another [MIT OCW's Calculus](https://ocw.mit.edu/courses/mathematics/18-01sc-single-variable-calculus-fall-2010/unit-2-applications-of-differentiation/) free course. I would recommend Part A and B for IST 718.
- **Probability and statistics** I would recommend the first chapters of DeGroot and Schervish's book "Probability and Statistics"
- **Programming**: There are plenty of resources online about programming. For programming in Python, I would recommend [Jake VanderPlas's "Python Data Science Handbook"](https://jakevdp.github.io/PythonDataScienceHandbook/).



### Goal 

This course is a broad introduction to modern techniques in data science including elastic net regularized regression, random forest, gradient boosting, and deep learning. It emphasizes a statistical learning point of view, and a careful examination of generalization error, model interpretability, feature engineering, and bias-variance tradeoff.

### Tools

The tool of choice is Apache Spark on Hadoop's HDFS. The environment we use is Databricks Community Edition, which runs a highly customized version of the Jupyter Notebook.  

### Prerequisites

The pre-requistes for this course are a basic knowledge of discrete mathematics, calculus, probability, and Python.

We use the following books:

1. [Python for Data Analysis (PFDA)](https://github.com/wesm/pydata-book), 2nd Edition
1. [An introduction to Statistical Learning with Applications in R (ISLR)](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf) by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani
1. [Spark: The Definitive Guide (STDG), Upcoming (expected 2018)](http://go.databricks.com/definitive-guide-apache-spark) by B. Chambers and M. Zaharia,
1. [Deep Learning (DL)](http://www.deeplearningbook.org/) by Ian Goodfellow, Yoshua Bengio, and Aaron Courville


### Syllabus

<iframe src="https://drive.google.com/file/d/1q-ibQUpdTxZQOUAU4bhn-F1BuXS3BXvS/preview" width="640" height="480"></iframe>